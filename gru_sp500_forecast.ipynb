{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gru_sp500_forecast.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPIsdY9ReefrtSej8QBbE9C",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sugiyama404/forecast_sp500/blob/main/gru_sp500_forecast.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4zxba3onBC4"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import sklearn\n",
        "import sklearn.preprocessing\n",
        "import datetime\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from google.colab import drive\n",
        "\n",
        "\n",
        "# split data in 80%/10%/10% train/validation/test sets\n",
        "valid_set_size_percentage = 10 \n",
        "test_set_size_percentage = 10 \n",
        "\n",
        "#display parent directory and working directory\n",
        "print(os.path.dirname(os.getcwd())+':', os.listdir(os.path.dirname(os.getcwd())));\n",
        "print(os.getcwd()+':', os.listdir(os.getcwd()));"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AzsV5-kHujxY"
      },
      "source": [
        "# import all stock prices \n",
        "\n",
        "drive.mount('/content/drive/')\n",
        "nov_dir = 'Colab Notebooks/dataset/'  # このフォルダへのパス\n",
        "nov_path = '/content/drive/My Drive/' + nov_dir + 'prices-split-adjusted.csv'\n",
        "\n",
        "df = pd.read_csv(nov_path, index_col = 0)\n",
        "df.info()\n",
        "df.head()\n",
        "\n",
        "# number of different stocks\n",
        "\n",
        "print('\\nnumber of different stocks: ', len(list(set(df.symbol))))\n",
        "print(list(set(df.symbol))[:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tGZRHJ8fu-WG"
      },
      "source": [
        "df.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CbklUEjl4iM-"
      },
      "source": [
        "df.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTmt7B3vRFAh"
      },
      "source": [
        "plt.figure(figsize=(15, 5));\n",
        "plt.subplot(1,2,1);\n",
        "plt.plot(df[df.symbol == 'EQIX'].open.values, color='red', label='open')\n",
        "plt.plot(df[df.symbol == 'EQIX'].close.values, color='green', label='close')\n",
        "plt.plot(df[df.symbol == 'EQIX'].low.values, color='blue', label='low')\n",
        "plt.plot(df[df.symbol == 'EQIX'].high.values, color='black', label='high')\n",
        "plt.title('stock price')\n",
        "plt.xlabel('time [days]')\n",
        "plt.ylabel('price')\n",
        "plt.legend(loc='best')\n",
        "#plt.show()\n",
        "\n",
        "plt.subplot(1,2,2);\n",
        "plt.plot(df[df.symbol == 'EQIX'].volume.values, color='black', label='volume')\n",
        "plt.title('stock volume')\n",
        "plt.xlabel('time [days]')\n",
        "plt.ylabel('volume')\n",
        "plt.legend(loc='best');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFxodMEkRUyl"
      },
      "source": [
        "# function for min-max normalization of stock\n",
        "def normalize_data(df):\n",
        "    min_max_scaler = sklearn.preprocessing.MinMaxScaler()\n",
        "    df['open'] = min_max_scaler.fit_transform(df.open.values.reshape(-1,1))\n",
        "    df['high'] = min_max_scaler.fit_transform(df.high.values.reshape(-1,1))\n",
        "    df['low'] = min_max_scaler.fit_transform(df.low.values.reshape(-1,1))\n",
        "    df['close'] = min_max_scaler.fit_transform(df['close'].values.reshape(-1,1))\n",
        "    return df\n",
        "\n",
        "# function to create train, validation, test data given stock data and sequence length\n",
        "def load_data(stock, seq_len):\n",
        "    #data_raw = stock.as_matrix() # convert to numpy array\n",
        "    data_raw = stock.values # convert to numpy array\n",
        "    data = []\n",
        "    \n",
        "    # create all possible sequences of length seq_len\n",
        "    for index in range(len(data_raw) - seq_len): \n",
        "        data.append(data_raw[index: index + seq_len])\n",
        "    \n",
        "    data = np.array(data);\n",
        "    valid_set_size = int(np.round(valid_set_size_percentage/100*data.shape[0]));  \n",
        "    test_set_size = int(np.round(test_set_size_percentage/100*data.shape[0]));\n",
        "    train_set_size = data.shape[0] - (valid_set_size + test_set_size);\n",
        "    \n",
        "    x_train = data[:train_set_size,:-1,:]\n",
        "    y_train = data[:train_set_size,-1,:]\n",
        "    \n",
        "    x_valid = data[train_set_size:train_set_size+valid_set_size,:-1,:]\n",
        "    y_valid = data[train_set_size:train_set_size+valid_set_size,-1,:]\n",
        "    \n",
        "    x_test = data[train_set_size+valid_set_size:,:-1,:]\n",
        "    y_test = data[train_set_size+valid_set_size:,-1,:]\n",
        "    \n",
        "    return [x_train, y_train, x_valid, y_valid, x_test, y_test]\n",
        "\n",
        "# choose one stock\n",
        "df_stock = df[df.symbol == 'EQIX'].copy()\n",
        "df_stock.drop(['symbol'],1,inplace=True)\n",
        "df_stock.drop(['volume'],1,inplace=True)\n",
        "\n",
        "cols = list(df_stock.columns.values)\n",
        "print('df_stock.columns.values = ', cols)\n",
        "\n",
        "# normalize stock\n",
        "df_stock_norm = df_stock.copy()\n",
        "df_stock_norm = normalize_data(df_stock_norm)\n",
        "\n",
        "# create train, test data\n",
        "seq_len = 20 # choose sequence length\n",
        "x_train, y_train, x_valid, y_valid, x_test, y_test = load_data(df_stock_norm, seq_len)\n",
        "print('x_train.shape = ',x_train.shape)\n",
        "print('y_train.shape = ', y_train.shape)\n",
        "print('x_valid.shape = ',x_valid.shape)\n",
        "print('y_valid.shape = ', y_valid.shape)\n",
        "print('x_test.shape = ', x_test.shape)\n",
        "print('y_test.shape = ',y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lY8wK5sjRbG1"
      },
      "source": [
        "plt.figure(figsize=(15, 5));\n",
        "plt.plot(df_stock_norm.open.values, color='red', label='open')\n",
        "plt.plot(df_stock_norm.close.values, color='green', label='low')\n",
        "plt.plot(df_stock_norm.low.values, color='blue', label='low')\n",
        "plt.plot(df_stock_norm.high.values, color='black', label='high')\n",
        "#plt.plot(df_stock_norm.volume.values, color='gray', label='volume')\n",
        "plt.title('stock')\n",
        "plt.xlabel('time [days]')\n",
        "plt.ylabel('normalized price/volume')\n",
        "plt.legend(loc='best')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MxfC8EqyR75s"
      },
      "source": [
        "## Basic Cell RNN in tensorflow\n",
        "\n",
        "index_in_epoch = 0;\n",
        "perm_array  = np.arange(x_train.shape[0])\n",
        "np.random.shuffle(perm_array)\n",
        "\n",
        "# function to get the next batch\n",
        "def get_next_batch(batch_size):\n",
        "    global index_in_epoch, x_train, perm_array   \n",
        "    start = index_in_epoch\n",
        "    index_in_epoch += batch_size\n",
        "    \n",
        "    if index_in_epoch > x_train.shape[0]:\n",
        "        np.random.shuffle(perm_array) # shuffle permutation array\n",
        "        start = 0 # start next epoch\n",
        "        index_in_epoch = batch_size\n",
        "        \n",
        "    end = index_in_epoch\n",
        "    return x_train[perm_array[start:end]], y_train[perm_array[start:end]]\n",
        "\n",
        "# parameters\n",
        "n_steps = seq_len-1 \n",
        "n_inputs = 4 \n",
        "n_neurons = 200 \n",
        "n_outputs = 4\n",
        "n_layers = 2\n",
        "learning_rate = 0.001\n",
        "batch_size = 50\n",
        "n_epochs = 100 \n",
        "train_set_size = x_train.shape[0]\n",
        "test_set_size = x_test.shape[0]\n",
        "\n",
        "# tf.reset_default_graph()\n",
        "tf.compat.v1.reset_default_graph()\n",
        "\n",
        "X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
        "y = tf.placeholder(tf.float32, [None, n_outputs])\n",
        "\n",
        "# use Basic RNN Cell\n",
        "layers = [tf.contrib.rnn.BasicRNNCell(num_units=n_neurons, activation=tf.nn.elu)\n",
        "          for layer in range(n_layers)]\n",
        "\n",
        "# use Basic LSTM Cell \n",
        "#layers = [tf.contrib.rnn.BasicLSTMCell(num_units=n_neurons, activation=tf.nn.elu)\n",
        "#          for layer in range(n_layers)]\n",
        "\n",
        "# use LSTM Cell with peephole connections\n",
        "#layers = [tf.contrib.rnn.LSTMCell(num_units=n_neurons, \n",
        "#                                  activation=tf.nn.leaky_relu, use_peepholes = True)\n",
        "#          for layer in range(n_layers)]\n",
        "\n",
        "# use GRU cell\n",
        "#layers = [tf.contrib.rnn.GRUCell(num_units=n_neurons, activation=tf.nn.leaky_relu)\n",
        "#          for layer in range(n_layers)]\n",
        "                                                                     \n",
        "multi_layer_cell = tf.contrib.rnn.MultiRNNCell(layers)\n",
        "rnn_outputs, states = tf.nn.dynamic_rnn(multi_layer_cell, X, dtype=tf.float32)\n",
        "\n",
        "stacked_rnn_outputs = tf.reshape(rnn_outputs, [-1, n_neurons]) \n",
        "stacked_outputs = tf.layers.dense(stacked_rnn_outputs, n_outputs)\n",
        "outputs = tf.reshape(stacked_outputs, [-1, n_steps, n_outputs])\n",
        "outputs = outputs[:,n_steps-1,:] # keep only last output of sequence\n",
        "                                              \n",
        "loss = tf.reduce_mean(tf.square(outputs - y)) # loss function = mean squared error \n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate) \n",
        "training_op = optimizer.minimize(loss)\n",
        "                                              \n",
        "# run graph\n",
        "with tf.Session() as sess: \n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    for iteration in range(int(n_epochs*train_set_size/batch_size)):\n",
        "        x_batch, y_batch = get_next_batch(batch_size) # fetch the next training batch \n",
        "        sess.run(training_op, feed_dict={X: x_batch, y: y_batch}) \n",
        "        if iteration % int(5*train_set_size/batch_size) == 0:\n",
        "            mse_train = loss.eval(feed_dict={X: x_train, y: y_train}) \n",
        "            mse_valid = loss.eval(feed_dict={X: x_valid, y: y_valid}) \n",
        "            print('%.2f epochs: MSE train/valid = %.6f/%.6f'%(\n",
        "                iteration*batch_size/train_set_size, mse_train, mse_valid))\n",
        "\n",
        "    y_train_pred = sess.run(outputs, feed_dict={X: x_train})\n",
        "    y_valid_pred = sess.run(outputs, feed_dict={X: x_valid})\n",
        "    y_test_pred = sess.run(outputs, feed_dict={X: x_test})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jv9URNaKQiZs"
      },
      "source": [
        "## 修正コード\n",
        "Model and validate data\n",
        "モデルとデータの検証\n",
        "RNNs with basic, LSTM, GRU cells\n",
        "\n",
        "\n",
        "reset_default_graph\n",
        "placeholder\n",
        "\n",
        "の二つを変換する。\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "# output code\n",
        "\n",
        "0.00 epochs: MSE train/valid = 0.125422/0.300375\n",
        "4.99 epochs: MSE train/valid = 0.000243/0.001049\n",
        "9.97 epochs: MSE train/valid = 0.000150/0.000744\n",
        "14.96 epochs: MSE train/valid = 0.000136/0.000558\n",
        "19.94 epochs: MSE train/valid = 0.000129/0.000432\n",
        "24.93 epochs: MSE train/valid = 0.000114/0.000543\n",
        "29.91 epochs: MSE train/valid = 0.000097/0.000431\n",
        "34.90 epochs: MSE train/valid = 0.000100/0.000294\n",
        "39.89 epochs: MSE train/valid = 0.000085/0.000304\n",
        "44.87 epochs: MSE train/valid = 0.000079/0.000278\n",
        "49.86 epochs: MSE train/valid = 0.000090/0.000395\n",
        "54.84 epochs: MSE train/valid = 0.000077/0.000322\n",
        "59.83 epochs: MSE train/valid = 0.000073/0.000234\n",
        "64.81 epochs: MSE train/valid = 0.000090/0.000426\n",
        "69.80 epochs: MSE train/valid = 0.000065/0.000251\n",
        "74.78 epochs: MSE train/valid = 0.000081/0.000363\n",
        "79.77 epochs: MSE train/valid = 0.000122/0.000311\n",
        "84.76 epochs: MSE train/valid = 0.000076/0.000278\n",
        "89.74 epochs: MSE train/valid = 0.000087/0.000380\n",
        "94.73 epochs: MSE train/valid = 0.000072/0.000275\n",
        "99.71 epochs: MSE train/valid = 0.000060/0.000211\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Cjaavi4QfnO"
      },
      "source": [
        "## Basic Cell RNN in tensorflow\n",
        "\n",
        "index_in_epoch = 0;\n",
        "perm_array  = np.arange(x_train.shape[0])\n",
        "np.random.shuffle(perm_array)\n",
        "\n",
        "# function to get the next batch\n",
        "def get_next_batch(batch_size):\n",
        "    global index_in_epoch, x_train, perm_array   \n",
        "    start = index_in_epoch\n",
        "    index_in_epoch += batch_size\n",
        "    \n",
        "    if index_in_epoch > x_train.shape[0]:\n",
        "        np.random.shuffle(perm_array) # shuffle permutation array\n",
        "        start = 0 # start next epoch\n",
        "        index_in_epoch = batch_size\n",
        "        \n",
        "    end = index_in_epoch\n",
        "    return x_train[perm_array[start:end]], y_train[perm_array[start:end]]\n",
        "\n",
        "# parameters\n",
        "n_steps = seq_len-1 \n",
        "n_inputs = 4 \n",
        "n_neurons = 200 \n",
        "n_outputs = 4\n",
        "n_layers = 2\n",
        "learning_rate = 0.001\n",
        "batch_size = 50\n",
        "n_epochs = 100 \n",
        "train_set_size = x_train.shape[0]\n",
        "test_set_size = x_test.shape[0]\n",
        "\n",
        "# tf.reset_default_graph()\n",
        "tf.compat.v1.reset_default_graph()\n",
        "\n",
        "X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
        "y = tf.placeholder(tf.float32, [None, n_outputs])\n",
        "\n",
        "# use Basic RNN Cell\n",
        "layers = [tf.contrib.rnn.BasicRNNCell(num_units=n_neurons, activation=tf.nn.elu)\n",
        "          for layer in range(n_layers)]\n",
        "\n",
        "# use Basic LSTM Cell \n",
        "#layers = [tf.contrib.rnn.BasicLSTMCell(num_units=n_neurons, activation=tf.nn.elu)\n",
        "#          for layer in range(n_layers)]\n",
        "\n",
        "# use LSTM Cell with peephole connections\n",
        "#layers = [tf.contrib.rnn.LSTMCell(num_units=n_neurons, \n",
        "#                                  activation=tf.nn.leaky_relu, use_peepholes = True)\n",
        "#          for layer in range(n_layers)]\n",
        "\n",
        "# use GRU cell\n",
        "#layers = [tf.contrib.rnn.GRUCell(num_units=n_neurons, activation=tf.nn.leaky_relu)\n",
        "#          for layer in range(n_layers)]\n",
        "                                                                     \n",
        "multi_layer_cell = tf.contrib.rnn.MultiRNNCell(layers)\n",
        "rnn_outputs, states = tf.nn.dynamic_rnn(multi_layer_cell, X, dtype=tf.float32)\n",
        "\n",
        "stacked_rnn_outputs = tf.reshape(rnn_outputs, [-1, n_neurons]) \n",
        "stacked_outputs = tf.layers.dense(stacked_rnn_outputs, n_outputs)\n",
        "outputs = tf.reshape(stacked_outputs, [-1, n_steps, n_outputs])\n",
        "outputs = outputs[:,n_steps-1,:] # keep only last output of sequence\n",
        "                                              \n",
        "loss = tf.reduce_mean(tf.square(outputs - y)) # loss function = mean squared error \n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate) \n",
        "training_op = optimizer.minimize(loss)\n",
        "                                              \n",
        "# run graph\n",
        "with tf.Session() as sess: \n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    for iteration in range(int(n_epochs*train_set_size/batch_size)):\n",
        "        x_batch, y_batch = get_next_batch(batch_size) # fetch the next training batch \n",
        "        sess.run(training_op, feed_dict={X: x_batch, y: y_batch}) \n",
        "        if iteration % int(5*train_set_size/batch_size) == 0:\n",
        "            mse_train = loss.eval(feed_dict={X: x_train, y: y_train}) \n",
        "            mse_valid = loss.eval(feed_dict={X: x_valid, y: y_valid}) \n",
        "            print('%.2f epochs: MSE train/valid = %.6f/%.6f'%(\n",
        "                iteration*batch_size/train_set_size, mse_train, mse_valid))\n",
        "\n",
        "    y_train_pred = sess.run(outputs, feed_dict={X: x_train})\n",
        "    y_valid_pred = sess.run(outputs, feed_dict={X: x_valid})\n",
        "    y_test_pred = sess.run(outputs, feed_dict={X: x_test})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_JvBi9UZs-Y"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "---------------------------------------------------------------------------\n",
        "AttributeError                            Traceback (most recent call last)\n",
        "<ipython-input-8-dd5d8552bc16> in <module>()\n",
        "     31 test_set_size = x_test.shape[0]\n",
        "     32 \n",
        "---> 33 tf.reset_default_graph()\n",
        "     34 \n",
        "     35 X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
        "\n",
        "AttributeError: module 'tensorflow' has no attribute 'reset_default_graph'\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "---------------------------------------------------------------------------\n",
        "AttributeError                            Traceback (most recent call last)\n",
        "<ipython-input-10-3dca69d2c155> in <module>()\n",
        "     35 ops.reset_default_graph()\n",
        "     36 \n",
        "---> 37 X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
        "     38 y = tf.placeholder(tf.float32, [None, n_outputs])\n",
        "     39 \n",
        "\n",
        "AttributeError: module 'tensorflow' has no attribute 'placeholder'\n",
        "```\n",
        "\n",
        "y_train_pred\n",
        "を求める。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JhEFM1DDSEd7"
      },
      "source": [
        "y_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CeoUQj-Er0V"
      },
      "source": [
        "ft = 0 # 0 = open, 1 = close, 2 = highest, 3 = lowest\n",
        "\n",
        "## show predictions\n",
        "plt.figure(figsize=(15, 5));\n",
        "plt.subplot(1,2,1);\n",
        "\n",
        "plt.plot(np.arange(y_train.shape[0]), y_train[:,ft], color='blue', label='train target')\n",
        "\n",
        "plt.plot(np.arange(y_train.shape[0], y_train.shape[0]+y_valid.shape[0]), y_valid[:,ft],\n",
        "         color='gray', label='valid target')\n",
        "\n",
        "plt.plot(np.arange(y_train.shape[0]+y_valid.shape[0],\n",
        "                   y_train.shape[0]+y_test.shape[0]+y_test.shape[0]),\n",
        "         y_test[:,ft], color='black', label='test target')\n",
        "\n",
        "plt.plot(np.arange(y_train_pred.shape[0]),y_train_pred[:,ft], color='red',\n",
        "         label='train prediction')\n",
        "\n",
        "plt.plot(np.arange(y_train_pred.shape[0], y_train_pred.shape[0]+y_valid_pred.shape[0]),\n",
        "         y_valid_pred[:,ft], color='orange', label='valid prediction')\n",
        "\n",
        "plt.plot(np.arange(y_train_pred.shape[0]+y_valid_pred.shape[0],\n",
        "                   y_train_pred.shape[0]+y_valid_pred.shape[0]+y_test_pred.shape[0]),\n",
        "         y_test_pred[:,ft], color='green', label='test prediction')\n",
        "\n",
        "plt.title('past and future stock prices')\n",
        "plt.xlabel('time [days]')\n",
        "plt.ylabel('normalized price')\n",
        "plt.legend(loc='best');\n",
        "\n",
        "plt.subplot(1,2,2);\n",
        "\n",
        "plt.plot(np.arange(y_train.shape[0], y_train.shape[0]+y_test.shape[0]),\n",
        "         y_test[:,ft], color='black', label='test target')\n",
        "\n",
        "plt.plot(np.arange(y_train_pred.shape[0], y_train_pred.shape[0]+y_test_pred.shape[0]),\n",
        "         y_test_pred[:,ft], color='green', label='test prediction')\n",
        "\n",
        "plt.title('future stock prices')\n",
        "plt.xlabel('time [days]')\n",
        "plt.ylabel('normalized price')\n",
        "plt.legend(loc='best');\n",
        "\n",
        "corr_price_development_train = np.sum(np.equal(np.sign(y_train[:,1]-y_train[:,0]),\n",
        "            np.sign(y_train_pred[:,1]-y_train_pred[:,0])).astype(int)) / y_train.shape[0]\n",
        "corr_price_development_valid = np.sum(np.equal(np.sign(y_valid[:,1]-y_valid[:,0]),\n",
        "            np.sign(y_valid_pred[:,1]-y_valid_pred[:,0])).astype(int)) / y_valid.shape[0]\n",
        "corr_price_development_test = np.sum(np.equal(np.sign(y_test[:,1]-y_test[:,0]),\n",
        "            np.sign(y_test_pred[:,1]-y_test_pred[:,0])).astype(int)) / y_test.shape[0]\n",
        "\n",
        "print('correct sign prediction for close - open price for train/valid/test: %.2f/%.2f/%.2f'%(\n",
        "    corr_price_development_train, corr_price_development_valid, corr_price_development_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytdLuQ78E27I"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}